{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FalaahArifKhan/RAI-summer-stability/blob/main/examples/Preprocessing_Techniques_Part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kE-QqGjbN4j_",
        "outputId": "bf6a0831-ca3e-4a0f-b102-9facc1d76100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'RAI-summer-stability'...\n",
            "remote: Enumerating objects: 453, done.\u001b[K\n",
            "remote: Total 453 (delta 0), reused 0 (delta 0), pack-reused 453\u001b[K\n",
            "Receiving objects: 100% (453/453), 52.93 MiB | 13.24 MiB/s, done.\n",
            "Resolving deltas: 100% (269/269), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/FalaahArifKhan/RAI-summer-stability.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFkz_rroNOfx"
      },
      "source": [
        "# Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "jO6zuK1iNjU2"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "try:\n",
        "    from folktables import ACSDataSource, ACSEmployment\n",
        "except:\n",
        "    !pip install folktables\n",
        "    from folktables import ACSDataSource, ACSEmployment\n",
        "    clear_output()\n",
        "from sys import getsizeof\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import mean_absolute_error as MAE\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
        "\n",
        "from utils.data_loader import *\n",
        "from utils.null_handler import *\n",
        "from utils.EDA_utils import *\n",
        "from utils.simple_utils import get_column_type\n",
        "from config import COLUMN_TO_TYPE, SEED\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "S1u3Hp8FNUOO"
      },
      "outputs": [],
      "source": [
        "X_data_load, y_data = ACSDataLoader(task=ACSEmployment, state=['AL'], year='2016')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6AxoQibRA5a",
        "outputId": "ae3dab97-f411-4cee-f654-7cc428e9a609"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: 5 mb\n",
            "Optimized: 2 mb\n"
          ]
        }
      ],
      "source": [
        "print(f'Original: {int(getsizeof(X_data_load) / 1024**2)} mb')\n",
        "print(f'Optimized: {int(getsizeof(optimize_ACSEmployment(X_data_load)) / 1024**2)} mb')\n",
        "\n",
        "X_data = optimize_ACSEmployment(X_data_load)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhdzoPkMRzgu",
        "outputId": "28f36bb7-75b8-4702-c96a-7d9af202a9e9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AGEP            0\n",
              "SCHL         1396\n",
              "MAR             0\n",
              "RELP            0\n",
              "DIS             0\n",
              "ESP         38956\n",
              "CIT             0\n",
              "MIG           444\n",
              "MIL          8820\n",
              "ANC             0\n",
              "NATIVITY        0\n",
              "DEAR            0\n",
              "DEYE            0\n",
              "DREM         2347\n",
              "SEX             0\n",
              "RAC1P           0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ],
      "source": [
        "X_data.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YA9t7UpTGrwD"
      },
      "source": [
        "# Imputation methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIz92qDA3BZa"
      },
      "source": [
        "## Deterministic or Stochastic Regression Imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "EPGlb-En3UFr"
      },
      "outputs": [],
      "source": [
        "def regression_imputation(input_data, column_names):\n",
        "    data = input_data.copy(deep=True)\n",
        "    for column_name in column_names:\n",
        "        column_type = get_column_type(column_name)\n",
        "\n",
        "        other_columns = [col for col in data.columns if col != column_name]\n",
        "        indexes = data[column_name].isna()\n",
        "        \n",
        "        not_null_df = data[~indexes]\n",
        "        null_df = data[indexes]\n",
        "\n",
        "        X_train = not_null_df[other_columns].to_numpy()\n",
        "        y_train = not_null_df[column_name].to_numpy()\n",
        "\n",
        "        X_pred = null_df[other_columns].to_numpy()\n",
        "        \n",
        "        if column_type == 'numerical':\n",
        "            model = LinearRegression().fit(X_train, y_train)\n",
        "        else:\n",
        "            model = LogisticRegression(multi_class='multinomial').fit(X_train, y_train)\n",
        "\n",
        "        data.loc[indexes, column_name] = model.predict(X_pred)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHUTGxHZDJTs"
      },
      "source": [
        "## kNN or Hot-Deck"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "Aes4JKTnDblw"
      },
      "outputs": [],
      "source": [
        "def kNN_imputation(input_data, column_names, n_neighbors=4, weights='distance'):\n",
        "    data = input_data.copy(deep=True)\n",
        "    for column_name in column_names:\n",
        "        column_type = get_column_type(column_name)\n",
        "\n",
        "        other_columns = [col for col in data.columns if col != column_name]\n",
        "        indexes = data[column_name].isna()\n",
        "        \n",
        "        not_null_df = data[~indexes]\n",
        "        null_df = data[indexes]\n",
        "\n",
        "        X_train = not_null_df[other_columns].to_numpy()\n",
        "        y_train = not_null_df[column_name].to_numpy()\n",
        "\n",
        "        X_pred = null_df[other_columns].to_numpy()\n",
        "        \n",
        "        if column_type == 'numerical':\n",
        "            model = KNeighborsRegressor(n_neighbors=n_neighbors, weights=weights).fit(X_train, y_train)\n",
        "        else:\n",
        "            model = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights).fit(X_train, y_train)\n",
        "\n",
        "        data.loc[indexes, column_name] = model.predict(X_pred)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding regression and kNN into handle_df_nulls"
      ],
      "metadata": {
        "id": "0R1nktwLdAIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_df_nulls(input_data, how, column_names, condition_column=None):\n",
        "    \"\"\"\n",
        "    Description: Processes the null values in the dataset\n",
        "    Input:\n",
        "    data: dataframe with missing values\n",
        "    how: processing method, currently supports\n",
        "            - 'special': corresponds to 'not applicable' scenario, designates null values as their own special category\n",
        "            - 'drop-column' : removes the column with nulls from the dataset\n",
        "            - 'drop-rows' : removes all the rows with the nulls values from the dataset\n",
        "            - 'predict-by-sklearn' : predict the values to impute nulls based on the features in the rows; used for multivariate data\n",
        "            - 'regression' : predict the values to impute with linear (logistic in categorical column) regression based\n",
        "            on all other dataset features;\n",
        "            - 'kNN' : predict the values with kNN regressor (classifier) with 5 neighbors and weighted by distance;\n",
        "            - 'impute-by-mode' : impute nulls by mode of the column values without nulls\n",
        "            - 'impute-by-mode-trimmed' : the same as 'impute-by-mode', but the column is filtered from nulls,\n",
        "            sorted in descending order, and top and bottom k% are removed from it. After that 'impute-by-mode' logic is applied\n",
        "            - 'impute-by-mean' : impute nulls by mean of the column values without nulls\n",
        "            - 'impute-by-mean-trimmed' : the same as 'impute-by-mean', but the column is filtered from nulls,\n",
        "            sorted in descending order, and top and bottom k% are removed from it. After that 'impute-by-mean' logic is applied\n",
        "            - 'impute-by-median' : impute nulls by median of the column values without nulls\n",
        "            - 'impute-by-median-trimmed' : the same as 'impute-by-median', but the column is filtered from nulls,\n",
        "            sorted in descending order, and top and bottom k% are removed from it. After that 'impute-by-median' logic is applied\n",
        "    column-names: list of column names, for which the particular techniques needs to be applied\n",
        "    Output:\n",
        "    dataframe with processed nulls\n",
        "    \"\"\"\n",
        "    data = input_data.copy(deep=True)\n",
        "\n",
        "    if how == 'drop-column':\n",
        "        data.drop(columns=column_names,  axis=1, inplace=True)\n",
        "    elif how == 'drop-rows':\n",
        "        data.dropna(subset=column_names, inplace=True)\n",
        "    elif how == 'predict-by-sklearn':\n",
        "        if len(column_names) > 1:\n",
        "            print(f\"\\n\\nERROR: {how} technique does not work with more than one column.\\n\\n\")\n",
        "            return data\n",
        "\n",
        "        # Setting the random_state argument for reproducibility\n",
        "        imputer = IterativeImputer(random_state=42,\n",
        "                                   min_value=input_data[column_names[0]].min(),\n",
        "                                   max_value=input_data[column_names[0]].max())\n",
        "        imputed = imputer.fit_transform(data)\n",
        "        data = pd.DataFrame(imputed, columns=data.columns)\n",
        "        data = data[column_names].round()\n",
        "    elif how == 'regression':\n",
        "        data = regression_imputation(data, column_names)\n",
        "    elif how == 'kNN':\n",
        "        data = kNN_imputation(data, column_names)\n",
        "    else:\n",
        "        get_impute_value = None\n",
        "        if how == 'special':\n",
        "            get_impute_value = decide_special_category\n",
        "        elif 'impute-by-mode' in how:\n",
        "            get_impute_value = find_column_mode\n",
        "        elif 'impute-by-mean' in how:\n",
        "            get_impute_value = find_column_mean\n",
        "        elif 'impute-by-median' in how:\n",
        "            get_impute_value = find_column_median\n",
        "\n",
        "        if 'conditional' in how:\n",
        "            data = apply_conditional_technique(data, column_names, condition_column, how, get_impute_value)\n",
        "        else:\n",
        "            vals = {}\n",
        "            for col in column_names:\n",
        "                filtered_df = data[~data[col].isnull()][[col]].copy(deep=True)\n",
        "                if 'trimmed' in how:\n",
        "                    k_percent = 10\n",
        "                    reduce_n_rows = int(filtered_df.shape[0] / 100 * k_percent)\n",
        "                    filtered_df.sort_values(by=[col], ascending=False, inplace=True)\n",
        "                    filtered_df = filtered_df[reduce_n_rows: -reduce_n_rows]\n",
        "\n",
        "                vals[col] = get_impute_value(filtered_df[col].values)\n",
        "            print(\"Impute values: \", vals)\n",
        "            data.fillna(value=vals, inplace=True)\n",
        "    return data"
      ],
      "metadata": {
        "id": "W-566-bCdUUp"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W19QtnzHGg4x"
      },
      "source": [
        "# Comparison of imputation methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "id": "BDhIgO6oMz3Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8737823-95c3-402c-ffa3-b9ad7c7355ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 206
        }
      ],
      "source": [
        "data_source = ACSDataSource(\n",
        "    survey_year='2016',\n",
        "    horizon='1-Year',\n",
        "    survey='person'\n",
        ")\n",
        "acs_data = data_source.get_data(states=['AL'], download=True)\n",
        "features, label, group = ACSEmployment.df_to_numpy(acs_data)\n",
        "\n",
        "X_full = pd.DataFrame(features, columns=ACSEmployment.features)\n",
        "y_full = pd.DataFrame(label)\n",
        "y_full.rename(columns={0: ACSEmployment.target}, inplace=True)\n",
        "X_full.isna().any().any()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate imputation"
      ],
      "metadata": {
        "id": "p34B73f7YTmm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_imputation(real, imputed, corrupted, column_names):\n",
        "    metrics = []\n",
        "    for column_name in column_names:\n",
        "        column_type = get_column_type(column_name)\n",
        "\n",
        "        indexes = corrupted[column_name].isna()\n",
        "        true = real.loc[indexes, column_name]\n",
        "        pred = imputed.loc[indexes, column_name]\n",
        "\n",
        "        if column_type == 'numerical':\n",
        "            mae = MAE(true, pred)\n",
        "            print('MAE for regression - {}: {:.1f}'.format(column_name, mae))\n",
        "            metrics.append(mae)\n",
        "        else:\n",
        "            conf_matrix = confusion_matrix(true, pred)\n",
        "            accuracy = conf_matrix.trace() / conf_matrix.sum()\n",
        "            print('Accuracy for regression - {}: {:.2f}'.format(column_name, accuracy))\n",
        "            metrics.append(accuracy)\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "RKBK5px4YZyp"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Null simulation and imputation"
      ],
      "metadata": {
        "id": "FAVodk1ckq2e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unknown. AGEP"
      ],
      "metadata": {
        "id": "i5rvx67nu2rw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_full.isna().any().any()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3P7iLARlkdr",
        "outputId": "e514358c-6f38-4900-e349-84db91e3cdda"
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "special_values = (8, 10, 11, 12, 15)\n",
        "condition_col = 'RELP'\n",
        "target_col = 'AGEP'\n",
        "fraction = .4\n",
        "corrupted_data_AGEP = nulls_simulator(X_full, target_col, condition_col, special_values, fraction)"
      ],
      "metadata": {
        "id": "Th4w--rwoP9Y"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = ['AGEP']\n",
        "\n",
        "imputed = handle_df_nulls(corrupted_data_AGEP, 'kNN', column_names)\n",
        "\n",
        "evaluate_imputation(X_full, imputed, corrupted_data_AGEP, column_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zD_S8yFEosLB",
        "outputId": "9c0a080b-bda1-4dd8-b1ff-93b99a704559"
      },
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE for regression - AGEP: 10.2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10.1880990699547]"
            ]
          },
          "metadata": {},
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Special. SEX"
      ],
      "metadata": {
        "id": "v1_dD8E6tA7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "special_values = [1]\n",
        "condition_col = 'SEX'\n",
        "target_col = 'SEX'\n",
        "fraction = .11\n",
        "corrupted_data_SEX = nulls_simulator(X_full, target_col, condition_col, special_values, fraction)"
      ],
      "metadata": {
        "id": "bOnV4s_TrH4l"
      },
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = ['SEX']\n",
        "\n",
        "imputed = handle_df_nulls(corrupted_data_SEX, 'kNN', column_names)\n",
        "\n",
        "evaluate_imputation(X_full, imputed, corrupted_data_SEX, column_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAZ8M1AfsANJ",
        "outputId": "ac7b9d3e-8bc0-45a0-cc37-9aa17754cc24"
      },
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for regression - SEX: 0.67\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6698412698412698]"
            ]
          },
          "metadata": {},
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optional MAR"
      ],
      "metadata": {
        "id": "vSP-g-JKBzxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "special_values = [2, 3, 4]\n",
        "condition_col='MAR'\n",
        "target_col='MAR'\n",
        "fraction=0.9\n",
        "corrupted_data_MAR = nulls_simulator(X_full, target_col, condition_col, special_values, fraction)"
      ],
      "metadata": {
        "id": "W5kCSgkbBvEu"
      },
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = ['MAR']\n",
        "\n",
        "imputed = handle_df_nulls(corrupted_data_MAR, 'kNN', column_names)\n",
        "\n",
        "evaluate_imputation(X_full, imputed, corrupted_data_MAR, column_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZlKidUBB6u7",
        "outputId": "eafe882b-5c1c-4045-b645-7e85d4eaeb13"
      },
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for regression - MAR: 0.03\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.02793090775450202]"
            ]
          },
          "metadata": {},
          "execution_count": 223
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Preprocessing_Techniques_Part2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOfnynpdTk0bImPgdxO+2ms",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}