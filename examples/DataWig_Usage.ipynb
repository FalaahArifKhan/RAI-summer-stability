{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "A5caSFfEhfhL"
   },
   "source": [
    "# DataWig usage for AGEP column"
   ],
   "id": "A5caSFfEhfhL"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W9eWFqbeiNX-",
    "outputId": "fd90534d-97f9-4fda-e1ba-5634e720aa8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2020 NVIDIA Corporation\n",
      "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
      "Cuda compilation tools, release 11.1, V11.1.105\n",
      "Build cuda_11.1.TC455_06.29190527_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ],
   "id": "W9eWFqbeiNX-"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dqOOee2Tm2jy"
   },
   "outputs": [],
   "source": [
    "# !wget https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb\n",
    "# !dpkg -i cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb\n",
    "# !apt-key add /var/cuda-repo-9-0-local/7fa2af80.pub\n",
    "# !apt-get update\n",
    "# !apt-get install cuda=9.0.176-1"
   ],
   "id": "dqOOee2Tm2jy"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PsfIl9fmpAd_"
   },
   "outputs": [],
   "source": [
    "# !apt-get update\n",
    "# !apt-get install cudnn=7.6.4-for-cuda-9.0"
   ],
   "id": "PsfIl9fmpAd_"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPsJj0fUpAHr"
   },
   "source": [
    ""
   ],
   "id": "HPsJj0fUpAHr"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZL-ous1AhqlS"
   },
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/awslabs/datawig/master/requirements/requirements.gpu-cu90.txt\n",
    "# !pip install datawig --no-deps -r requirements.gpu-cu90.txt\n",
    "# !rm requirements.gpu-cu90.txt"
   ],
   "id": "ZL-ous1AhqlS"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7uVBalWjuqNG"
   },
   "source": [
    "Below cell helped very much to start DataWig"
   ],
   "id": "7uVBalWjuqNG"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19kbl7gCtqXl",
    "outputId": "3d8f7be9-87d5-4996-c692-2b7d9877ada6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting mxnet-cu110\n",
      "  Downloading mxnet_cu110-1.9.1-py3-none-manylinux2014_x86_64.whl (327.3 MB)\n",
      "\u001B[K     |████████████████████████████████| 327.3 MB 5.7 kB/s \n",
      "\u001B[?25hCollecting graphviz<0.9.0,>=0.8.1\n",
      "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet-cu110) (2.23.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet-cu110) (1.21.6)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu110) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu110) (2022.6.15)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu110) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu110) (2.10)\n",
      "Installing collected packages: graphviz, mxnet-cu110\n",
      "  Attempting uninstall: graphviz\n",
      "    Found existing installation: graphviz 0.10.1\n",
      "    Uninstalling graphviz-0.10.1:\n",
      "      Successfully uninstalled graphviz-0.10.1\n",
      "Successfully installed graphviz-0.8.4 mxnet-cu110-1.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install mxnet-cu110"
   ],
   "id": "19kbl7gCtqXl"
  },
  {
   "cell_type": "code",
   "source": [
    "!!pip install datawig --no-deps"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZnGpFUnB6m_s",
    "outputId": "b9cbd238-3a65-430c-be03-541cfe425588"
   },
   "id": "ZnGpFUnB6m_s",
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/',\n",
       " 'Collecting datawig',\n",
       " '  Downloading datawig-0.2.0.tar.gz (61 kB)',\n",
       " '\\x1b[?25l',\n",
       " '\\x1b[K     |█████▎                          | 10 kB 33.2 MB/s eta 0:00:01',\n",
       " '\\x1b[K     |██████████▋                     | 20 kB 40.0 MB/s eta 0:00:01',\n",
       " '\\x1b[K     |████████████████                | 30 kB 41.0 MB/s eta 0:00:01',\n",
       " '\\x1b[K     |█████████████████████▎          | 40 kB 43.8 MB/s eta 0:00:01',\n",
       " '\\x1b[K     |██████████████████████████▋     | 51 kB 34.8 MB/s eta 0:00:01',\n",
       " '\\x1b[K     |████████████████████████████████| 61 kB 27.7 MB/s eta 0:00:01',\n",
       " '\\x1b[K     |████████████████████████████████| 61 kB 45 kB/s ',\n",
       " '\\x1b[?25hBuilding wheels for collected packages: datawig',\n",
       " '  Building wheel for datawig (setup.py) ... \\x1b[?25l\\x1b[?25hdone',\n",
       " '  Created wheel for datawig: filename=datawig-0.2.0-py3-none-any.whl size=72679 sha256=6a913ec42454a5f6a9159b73762dba01fd296cbcde49b15e851cd7d96f1ed91b',\n",
       " '  Stored in directory: /root/.cache/pip/wheels/23/44/aa/12cf6e868f0d71e3c4e57796330094461ade4cb3f1a3acd6c4',\n",
       " 'Successfully built datawig',\n",
       " 'Installing collected packages: datawig',\n",
       " 'Successfully installed datawig-0.2.0']"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "In case you train models in Google Colab, use the below cell to connect to your Google Drive with datasets.\n"
   ],
   "metadata": {
    "id": "DFjvB2-PBtbv"
   },
   "id": "DFjvB2-PBtbv"
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ThwooDDaBqtH",
    "outputId": "ba2214e0-dc48-4251-ec2e-4d83c5f35840"
   },
   "id": "ThwooDDaBqtH",
   "execution_count": 55,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "qmBnYNlNhfhP"
   },
   "source": [
    "Enable auto-reloading of external modules"
   ],
   "id": "qmBnYNlNhfhP"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_9gxH1jdhfhR",
    "outputId": "52bda729-c073-4fd4-ef52-681b4ad41ddf"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "_9gxH1jdhfhR"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "meQm-7zNhfhS"
   },
   "source": [
    "Import dependencies"
   ],
   "id": "meQm-7zNhfhS"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "E7hIHO8thfhT"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\""
   ],
   "id": "E7hIHO8thfhT"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "pnOzRTz-hfhV"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from folktables import ACSDataSource, ACSEmployment\n",
    "except:\n",
    "    !pip install folktables\n",
    "    from folktables import ACSDataSource, ACSEmployment"
   ],
   "id": "pnOzRTz-hfhV"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e34f792f"
   },
   "source": [
    "## Loading ACSEmployment data"
   ],
   "id": "e34f792f"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "_hLCjpQ8hfhX"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sys import getsizeof\n",
    "from folktables import ACSDataSource\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.experimental import enable_iterative_imputer # Required for IterativeImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "SEED=10\n",
    "COLUMN_TO_TYPE = {\n",
    "    \"categorical\": ['MAR', 'MIL', 'ESP', 'MIG', 'DREM', 'NATIVITY', 'DIS', 'DEAR', 'DEYE', 'SEX', 'RAC1P', 'RELP', 'CIT', 'ANC'],\n",
    "    \"numerical\": ['SCHL', 'AGEP']\n",
    "}\n",
    "\n",
    "\n",
    "def get_column_type(column_name):\n",
    "    for column_type in COLUMN_TO_TYPE.keys():\n",
    "        if column_name in COLUMN_TO_TYPE[column_type]:\n",
    "            return column_type\n",
    "    return None\n",
    "\n",
    "\n",
    "def evaluate_imputation(real, imputed, corrupted, column_names):\n",
    "    metrics = []\n",
    "    for column_name in column_names:\n",
    "        column_type = get_column_type(column_name)\n",
    "\n",
    "        indexes = corrupted[column_name].isna()\n",
    "        true = real.loc[indexes, column_name]\n",
    "        pred = imputed.loc[indexes, column_name]\n",
    "\n",
    "        if column_type == 'numerical':\n",
    "            mae = MAE(true, pred)\n",
    "            print('MAE for regression - {}: {:.1f}'.format(column_name, mae))\n",
    "            metrics.append(mae)\n",
    "        else:\n",
    "            conf_matrix = confusion_matrix(true, pred)\n",
    "            accuracy = conf_matrix.trace() / conf_matrix.sum()\n",
    "            print('Accuracy for regression - {}: {:.2f}'.format(column_name, accuracy))\n",
    "            metrics.append(accuracy)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def impute_df_with_all_techniques(real_data, corrupted_data, target_column, column_type, null_scenario_name, enable_plots=False):\n",
    "    \"\"\"\n",
    "    Impute target_column in corrupted_data with appropriate techniques.\n",
    "\n",
    "    :param real_data: an original dataset without nulls\n",
    "    :param corrupted_data: a corrupted dataset with nulls, created based on one of null scenarios\n",
    "    :param target_column: a column in corrupted_data, which has simulated nulls\n",
    "    :param column_type: categorical or numerical, a type of target_column\n",
    "    :param null_scenario_name: a name of null simulation method\n",
    "    :param enable_plots: bool, if display plots for analysis or not\n",
    "\n",
    "    :return: a dict, where a key is a name of an imputation technique, value is an imputed datasets with respective techniques\n",
    "    \"\"\"\n",
    "    if column_type == \"categorical\":\n",
    "        how_to_list = [\"drop-column\", \"drop-rows\", \"predict-by-sklearn\", \"kNN\", \"regression\",\n",
    "                       \"impute-by-mode\", \"impute-by-mode-trimmed\", \"impute-by-mode-conditional\"]\n",
    "    elif column_type == \"numerical\":\n",
    "        how_to_list = [\"drop-column\", \"drop-rows\", \"predict-by-sklearn\", \"kNN\", \"regression\",\n",
    "                       \"impute-by-mean\", \"impute-by-mean-trimmed\", \"impute-by-mean-conditional\",\n",
    "                       \"impute-by-median\", \"impute-by-median-trimmed\", \"impute-by-median-conditional\"]\n",
    "    else:\n",
    "        raise ValueError(\"Incorrect input column_type. It must be in ('categorical', 'numerical')\")\n",
    "\n",
    "    # Set style for seaborn plots\n",
    "    sns.set_style(\"darkgrid\")\n",
    "\n",
    "    # Save a result imputed dataset in imputed_data_dict for each imputation technique\n",
    "    imputed_data_dict = dict()\n",
    "    technique_metrics_dict = dict()\n",
    "    for how_to in how_to_list:\n",
    "        print(\"\\n\" * 4, \"#\" * 15, f\" Impute {target_column} column with {how_to} technique \", \"#\" * 15)\n",
    "        imputed_data = None\n",
    "        if 'conditional' in how_to and how_to not in (\"drop-column\", \"drop-rows\"):\n",
    "            for condition_column in [\"SEX\", \"RAC1P\"]:\n",
    "                # When condition_column == target_column, imputation based on subgroups does not make sense\n",
    "                if condition_column == target_column:\n",
    "                    continue\n",
    "                imputed_data = handle_df_nulls(corrupted_data,\n",
    "                                               how_to,\n",
    "                                               condition_column=condition_column,\n",
    "                                               column_names=[target_column])\n",
    "                # Measure imputation metrics\n",
    "                metrics = evaluate_imputation(real_data, imputed_data, corrupted_data, [target_column])\n",
    "                technique_metrics_dict[f'{how_to}_{condition_column}'] = metrics[0]\n",
    "\n",
    "                imputed_data_dict[f'{how_to}_{condition_column}'] = imputed_data\n",
    "\n",
    "        else:\n",
    "            imputed_data = handle_df_nulls(corrupted_data,\n",
    "                                           how_to,\n",
    "                                           condition_column=None,\n",
    "                                           column_names=[target_column])\n",
    "            # Measure imputation metrics. We can not measure them for \"drop-column\" and \"drop-rows\" techniques\n",
    "            if how_to not in (\"drop-column\", \"drop-rows\"):\n",
    "                metrics = evaluate_imputation(real_data, imputed_data, corrupted_data, [target_column])\n",
    "                technique_metrics_dict[how_to] = metrics[0]\n",
    "            imputed_data_dict[how_to] = imputed_data\n",
    "\n",
    "        # # Make plots for other techniques except \"drop-column\", since we dropped the column based on this technique\n",
    "        # if enable_plots and how_to != \"drop-column\":\n",
    "        #     imputed_nulls_analysis(real_data, imputed_data, corrupted_data, target_col=target_column)\n",
    "\n",
    "    # Save metrics of imputations techniques to a .json file for future analysis\n",
    "    technique_metrics_dict = {k: v for k, v in sorted(technique_metrics_dict.items(), key=lambda item: item[1])}\n",
    "    with open(os.path.join('..', 'results', null_scenario_name, f'{null_scenario_name}_imputation_techniques_metrics.json'), 'w') as f:\n",
    "        json.dump(technique_metrics_dict, f, indent=4)\n",
    "    return imputed_data_dict, technique_metrics_dict\n",
    "\n",
    "\n",
    "def handle_df_nulls(input_data, how, column_names, condition_column=None):\n",
    "    \"\"\"\n",
    "    Description: Processes the null values in the dataset\n",
    "    Input:\n",
    "    data: dataframe with missing values\n",
    "    how: processing method, currently supports\n",
    "        - 'special': corresponds to 'not applicable' scenario, designates null values as their own special category\n",
    "        - 'drop-column' : removes the column with nulls from the dataset\n",
    "        - 'drop-rows' : removes all the rows with the nulls values from the dataset\n",
    "        - 'predict-by-sklearn' : predict values to impute nulls based on the features in the rows; used for multivariate data\n",
    "        - 'impute-by-mode' : impute nulls by mode of the column values without nulls\n",
    "        - 'impute-by-mode-trimmed' : the same as 'impute-by-mode', but the column is filtered from nulls,\n",
    "        sorted in descending order, and top and bottom k% are removed from it. After that 'impute-by-mode' logic is applied\n",
    "        - 'impute-by-mean' : impute nulls by mean of the column values without nulls\n",
    "        - 'impute-by-mean-trimmed' : the same as 'impute-by-mean', but the column is filtered from nulls,\n",
    "        sorted in descending order, and top and bottom k% are removed from it. After that 'impute-by-mean' logic is applied\n",
    "        - 'impute-by-median' : impute nulls by median of the column values without nulls\n",
    "        - 'impute-by-median-trimmed' : the same as 'impute-by-median', but the column is filtered from nulls,\n",
    "        sorted in descending order, and top and bottom k% are removed from it. After that 'impute-by-median' logic is applied\n",
    "        - 'impute-by-(mode/mean/median)-conditional' : the same as 'impute-by-(mode/mean/median)',\n",
    "        but (mode/mean/median) is counted for each group and each group is imputed with this appropriate (mode/mean/median).\n",
    "        Groups are created based on split of a dataset by RAC1P or SEX\n",
    "    column-names: list of column names, for which the particular techniques needs to be applied\n",
    "\n",
    "    Output:\n",
    "    a dataframe with processed nulls\n",
    "    \"\"\"\n",
    "    data = input_data.copy(deep=True)\n",
    "\n",
    "    if how == 'drop-column':\n",
    "        data.drop(columns=column_names,  axis=1, inplace=True)\n",
    "    elif how == 'drop-rows':\n",
    "        data.dropna(subset=column_names, inplace=True)\n",
    "    elif how == 'predict-by-sklearn':\n",
    "        if len(column_names) > 1:\n",
    "            print(f\"\\n\\nERROR: {how} technique does not work with more than one column.\\n\\n\")\n",
    "            return data\n",
    "\n",
    "        # Setting the random_state argument for reproducibility\n",
    "        imputer = IterativeImputer(random_state=SEED,\n",
    "                                   min_value=input_data[column_names[0]].min(),\n",
    "                                   max_value=input_data[column_names[0]].max())\n",
    "        imputed = imputer.fit_transform(data)\n",
    "        data = pd.DataFrame(imputed, columns=data.columns)\n",
    "    elif how == 'regression':\n",
    "        data = regression_imputation(data, column_names)\n",
    "    elif how == 'kNN':\n",
    "        data = kNN_imputation(data, column_names)\n",
    "    else:\n",
    "        get_impute_value = None\n",
    "        if how == 'special':\n",
    "            get_impute_value = decide_special_category\n",
    "        elif 'impute-by-mode' in how:\n",
    "            get_impute_value = find_column_mode\n",
    "        elif 'impute-by-mean' in how:\n",
    "            get_impute_value = find_column_mean\n",
    "        elif 'impute-by-median' in how:\n",
    "            get_impute_value = find_column_median\n",
    "\n",
    "        if 'conditional' in how:\n",
    "            data = apply_conditional_technique(data, column_names, condition_column, how, get_impute_value)\n",
    "        else:\n",
    "            vals = {}\n",
    "            for col in column_names:\n",
    "                filtered_df = data[~data[col].isnull()][[col]].copy(deep=True)\n",
    "                if 'trimmed' in how:\n",
    "                    k_percent = 10\n",
    "                    reduce_n_rows = int(filtered_df.shape[0] / 100 * k_percent)\n",
    "                    filtered_df.sort_values(by=[col], ascending=False, inplace=True)\n",
    "                    filtered_df = filtered_df[reduce_n_rows: -reduce_n_rows]\n",
    "\n",
    "                vals[col] = get_impute_value(filtered_df[col].values)\n",
    "            print(\"Impute values: \", vals)\n",
    "            data.fillna(value=vals, inplace=True)\n",
    "\n",
    "    if how != 'drop-column':\n",
    "        data[column_names] = data[column_names].round()\n",
    "    return data\n",
    "\n",
    "\n",
    "def apply_conditional_technique(data, column_names, condition_column, how, get_impute_value):\n",
    "    \"\"\"\n",
    "    Function is used in handle_df_nulls() for 'impute-by-(mode/mean/median)-conditional' imputation techniques.\n",
    "    It imputes nulls with mean/mode/median in the input dataset for each group.\n",
    "    Groups are created based on split of a dataset by condition_column (RAC1P or SEX).\n",
    "\n",
    "    :param data: a dataset for imputation\n",
    "    :param column_names: column names to impute\n",
    "    :param condition_column: a conditional column based on which the dataset is split on groups.\n",
    "     Data in each of these groups is imputed with appropriate mean/mode/median.\n",
    "     In general RAC1P or SEX, but it can be any column, except those, which are in column_names.\n",
    "    :param how: a name of imputation technique\n",
    "    :param get_impute_value: a function like find_column_mean or find_column_mode etc.,\n",
    "    which is used to get values for the nulls imputation\n",
    "\n",
    "    :return: a dataframe with processed nulls\n",
    "    \"\"\"\n",
    "    for col in column_names:\n",
    "        filtered_df = data[~data[col].isnull()][[col, condition_column]].copy(deep=True)\n",
    "        if col == condition_column:\n",
    "            print(f\"\\n\\n\\nERROR: a target column from column_names list can not be equal to conditional column. \"\n",
    "                  f\"Skip {how} technique for {col} column\\n\\n\\n\")\n",
    "            continue\n",
    "\n",
    "        # When condition_column = 'AGEP', we want to create two groups based on threshold_age.\n",
    "        # To split based on AGEP we need to create a technical column AGEP_categorical, which value is 0 or 1.\n",
    "        # 0 in AGEP_categorical means that value in AGEP column is < threshold_age;\n",
    "        # 1 in AGEP_categorical means that value in AGEP column is >= threshold_age\n",
    "        if condition_column == 'AGEP':\n",
    "            threshold_age = 40\n",
    "            filtered_df[condition_column + '_categorical'] = filtered_df[condition_column].apply(lambda x: int(x >= threshold_age))\n",
    "            data[condition_column + '_categorical'] = data[condition_column].apply(lambda x: int(x >= threshold_age))\n",
    "            condition_column = condition_column + '_categorical'\n",
    "\n",
    "        # For each group add a value, which will be used to impute, to mapping_dict.\n",
    "        # Groups are splits of the input dataset based on condition_column\n",
    "        mapping_dict = dict()\n",
    "        for val in filtered_df[condition_column].unique():\n",
    "            fillna_val = get_impute_value(filtered_df[filtered_df[condition_column] == val][col].values)\n",
    "            print(f\"Impute {col} with value {fillna_val}, where {condition_column} == {val}\")\n",
    "            mapping_dict[val] = fillna_val\n",
    "\n",
    "        missing_mask = data[col].isna()\n",
    "        data.loc[missing_mask, col] = data.loc[missing_mask, condition_column].map(mapping_dict)\n",
    "        # Remove the technical column\n",
    "        if condition_column == 'AGEP_categorical':\n",
    "            data.drop(condition_column, axis=1, inplace=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "def regression_imputation(input_data, column_names):\n",
    "    data = input_data.copy(deep=True)\n",
    "    for column_name in column_names:\n",
    "        column_type = get_column_type(column_name)\n",
    "\n",
    "        other_columns = [col for col in data.columns if col != column_name]\n",
    "        indexes = data[column_name].isna()\n",
    "\n",
    "        not_null_df = data[~indexes]\n",
    "        null_df = data[indexes]\n",
    "\n",
    "        X_train = not_null_df[other_columns].to_numpy()\n",
    "        y_train = not_null_df[column_name].to_numpy()\n",
    "\n",
    "        X_pred = null_df[other_columns].to_numpy()\n",
    "\n",
    "        if column_type == 'numerical':\n",
    "            model = LinearRegression().fit(X_train, y_train)\n",
    "        else:\n",
    "            model = LogisticRegression(multi_class='multinomial').fit(X_train, y_train)\n",
    "\n",
    "        data.loc[indexes, column_name] = model.predict(X_pred)\n",
    "    return data\n",
    "\n",
    "\n",
    "def kNN_imputation(input_data, column_names, n_neighbors=4, weights='distance'):\n",
    "    data = input_data.copy(deep=True)\n",
    "    for column_name in column_names:\n",
    "        column_type = get_column_type(column_name)\n",
    "\n",
    "        other_columns = [col for col in data.columns if col != column_name]\n",
    "        indexes = data[column_name].isna()\n",
    "\n",
    "        not_null_df = data[~indexes]\n",
    "        null_df = data[indexes]\n",
    "\n",
    "        X_train = not_null_df[other_columns].to_numpy()\n",
    "        y_train = not_null_df[column_name].to_numpy()\n",
    "\n",
    "        X_pred = null_df[other_columns].to_numpy()\n",
    "\n",
    "        if column_type == 'numerical':\n",
    "            model = KNeighborsRegressor(n_neighbors=n_neighbors, weights=weights).fit(X_train, y_train)\n",
    "        else:\n",
    "            model = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights).fit(X_train, y_train)\n",
    "\n",
    "        data.loc[indexes, column_name] = model.predict(X_pred)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_sample_rows(data, target_col, fraction):\n",
    "    \"\"\"\n",
    "    Description: create a list of random indexes for rows, which will be used to place nulls in the dataset\n",
    "    \"\"\"\n",
    "    n_values_to_discard = int(len(data) * min(fraction, 1.0))\n",
    "    perc_lower_start = np.random.randint(0, len(data) - n_values_to_discard)\n",
    "    perc_idx = range(perc_lower_start, perc_lower_start + n_values_to_discard)\n",
    "\n",
    "    depends_on_col = np.random.choice(list(set(data.columns) - {target_col}))\n",
    "    # Pick a random percentile of values in other column\n",
    "    rows = data[depends_on_col].sort_values().iloc[perc_idx].index\n",
    "    return rows\n",
    "\n",
    "\n",
    "def simulate_scenario(data, simulated_scenario_dict):\n",
    "    return nulls_simulator(data,\n",
    "                           simulated_scenario_dict['target_col'],\n",
    "                           simulated_scenario_dict['condition_col'],\n",
    "                           simulated_scenario_dict['special_values'],\n",
    "                           simulated_scenario_dict['fraction'])\n",
    "\n",
    "\n",
    "def nulls_simulator(data, target_col, condition_col, special_values, fraction, nan_value=np.nan):\n",
    "    \"\"\"\n",
    "    Description: simulate nulls for the target column in the dataset based on the condition column and its special values.\n",
    "\n",
    "    Input:\n",
    "    :param data: a pandas dataframe, in which nulls values should be simulated\n",
    "    :param target_col: a column in the dataset, in which nulls should be placed\n",
    "    :param condition_col: a column in the dataset based on which null location should be identified\n",
    "    :param special_values: list of special values for the condition column; special_values and condition_col state the condition,\n",
    "        where nulls should be placed\n",
    "    :param fraction: float in range [0.0, 1.0], fraction of nulls, which should be placed based on the condition\n",
    "    :param nan_value: a value, which should be used as null to be placed in the dataset\n",
    "\n",
    "    Output: a dataset with null values based on the condition and fraction\n",
    "    \"\"\"\n",
    "    if target_col not in data.columns:\n",
    "        return ValueError(f'Column {target_col} does not exist in the dataset')\n",
    "    if condition_col not in data.columns:\n",
    "        return ValueError(f'Column {condition_col} does not exist in the dataset')\n",
    "    if not (0 <= fraction <= 1):\n",
    "        return ValueError(f'Fraction {fraction} is not in range [0, 1]')\n",
    "\n",
    "    corrupted_data = data[data[condition_col].isin(special_values)].copy(deep=True)\n",
    "    rows = get_sample_rows(corrupted_data, target_col, fraction)\n",
    "    corrupted_data.loc[rows, [target_col]] = nan_value\n",
    "    corrupted_data = pd.concat([corrupted_data, data[~data[condition_col].isin(special_values)]], axis=0)\n",
    "    return corrupted_data\n",
    "\n",
    "\n",
    "def decide_special_category(data):\n",
    "    \"\"\"\n",
    "    Description: Decides which value to designate as a special value, based on the values already in the data\n",
    "    \"\"\"\n",
    "    if 0 not in data:\n",
    "        return 0\n",
    "    else:\n",
    "        return max(data) + 1\n",
    "\n",
    "\n",
    "def find_column_mode(data):\n",
    "    result = stats.mode(data)\n",
    "    return result.mode[0]\n",
    "\n",
    "\n",
    "def find_column_mean(data):\n",
    "    return np.mean(data).round()\n",
    "\n",
    "\n",
    "def find_column_median(data):\n",
    "    return np.median(data).round()\n",
    "\n",
    "\n",
    "def ACSDataLoader(task, state, year, without_nulls):\n",
    "    '''\n",
    "    Loading task data: instead of using the task wrapper, we subsample the acs_data dataframe on the task features\n",
    "    We do this to retain the nulls as task wrappers handle nulls by imputing as a special category\n",
    "    Alternatively, we could have altered the configuration from here:\n",
    "    https://github.com/zykls/folktables/blob/main/folktables/acs.py\n",
    "    '''\n",
    "    data_source = ACSDataSource(\n",
    "        survey_year=year,\n",
    "        horizon='1-Year',\n",
    "        survey='person'\n",
    "    )\n",
    "    acs_data = data_source.get_data(states=state, download=True)\n",
    "    X = acs_data[task.features]\n",
    "    y = acs_data[task.target].apply(lambda x: int(x == 1))\n",
    "\n",
    "    # If the task is ACSEmployment, we can optimize the file size\n",
    "    print(f'Original: {int(getsizeof(X) / 1024**2)} mb')\n",
    "    X_data = optimize_ACSEmployment(X)\n",
    "    print(f'Optimized: {int(getsizeof(optimize_ACSEmployment(X_data)) / 1024**2)} mb\\n')\n",
    "\n",
    "    if without_nulls:\n",
    "        # Encode initial nulls in the dataset as a separate category, what was proved in EDA/EDA_CA_2016.ipynb notebook\n",
    "        missing = ['SCHL', 'ESP', 'MIG', 'MIL', 'DREM']\n",
    "        X_data = initially_handle_nulls(X_data, missing)\n",
    "        # Rechecking if there are nulls -- if the null_handler has run correctly, there should not be\n",
    "        print('\\nRechecking if there are nulls in X_data:')\n",
    "        print(X_data.isnull().sum())\n",
    "\n",
    "    return X_data, y\n",
    "\n",
    "def initially_handle_nulls(X_data, missing):\n",
    "    handle_nulls = {\n",
    "        'special': missing,\n",
    "    }\n",
    "    # Checking dataset shape before handling nulls\n",
    "    print(\"Dataset shape before handling nulls: \", X_data.shape)\n",
    "\n",
    "    for how_to in handle_nulls.keys():\n",
    "        X_data = handle_df_nulls(X_data, how_to, handle_nulls[how_to])\n",
    "    # Checking dataset shape after handling nulls\n",
    "    print(\"Dataset shape after handling nulls: \", X_data.shape)\n",
    "    return X_data\n",
    "  \n",
    "\n",
    "def optimize_ACSEmployment(data):\n",
    "    '''\n",
    "    Optimizing the dataset size by downcasting categorical columns\n",
    "    '''\n",
    "    categorical = ['SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX']\n",
    "    for column in categorical:\n",
    "        data[column] = pd.to_numeric(data[column], downcast='integer')\n",
    "    return data\n"
   ],
   "id": "_hLCjpQ8hfhX"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "32820cac"
   },
   "outputs": [],
   "source": [
    "TARGET_COLUMN = 'AGEP'\n",
    "NULL_SCENARIO_NAME = f'Unknown_{TARGET_COLUMN}'\n",
    "COLUMN_TYPE = 'numerical'"
   ],
   "id": "32820cac"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "d3g8Vlgdhfha"
   },
   "outputs": [],
   "source": [
    "DATASET_CONFIG = {\n",
    "    'state': [\"NY\"],\n",
    "    'year': '2018',\n",
    "    'task': ACSEmployment\n",
    "}"
   ],
   "id": "d3g8Vlgdhfha"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7067c19",
    "outputId": "b490f724-1362-4852-e5db-56fb08bc7f47"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original: 24 mb\n",
      "Optimized: 12 mb\n",
      "\n",
      "Dataset shape before handling nulls:  (196967, 16)\n",
      "Impute values:  {'SCHL': 0, 'ESP': 0, 'MIG': 0, 'MIL': 0, 'DREM': 0}\n",
      "Dataset shape after handling nulls:  (196967, 16)\n",
      "\n",
      "Rechecking if there are nulls in X_data:\n",
      "AGEP        0\n",
      "SCHL        0\n",
      "MAR         0\n",
      "RELP        0\n",
      "DIS         0\n",
      "ESP         0\n",
      "CIT         0\n",
      "MIG         0\n",
      "MIL         0\n",
      "ANC         0\n",
      "NATIVITY    0\n",
      "DEAR        0\n",
      "DEYE        0\n",
      "DREM        0\n",
      "SEX         0\n",
      "RAC1P       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_data, y_data = ACSDataLoader(task=DATASET_CONFIG['task'], state=DATASET_CONFIG['state'], year=DATASET_CONFIG['year'], without_nulls=True)"
   ],
   "id": "d7067c19"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C2klw8Echfhc",
    "outputId": "57177b40-dbe7-4745-ca3f-96aa99873ca3"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    105498\n",
       "1     91469\n",
       "Name: ESR, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "y_data.value_counts()"
   ],
   "id": "C2klw8Echfhc"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "225fbb72"
   },
   "source": [
    "## Simulate Null Scenarios"
   ],
   "id": "225fbb72"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "P0AIHfUwhfhh"
   },
   "outputs": [],
   "source": [
    "SIMULATED_SCENARIOS_DICT = {\n",
    "    'special_values': (8, 10, 11, 12, 15),\n",
    "    'condition_col': 'RELP',\n",
    "    'target_col': 'AGEP',\n",
    "    'fraction': 0.3\n",
    "}\n",
    "corrupted_data = simulate_scenario(X_data, SIMULATED_SCENARIOS_DICT)"
   ],
   "id": "P0AIHfUwhfhh"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "t3qCixEzhfhi"
   },
   "source": [
    "## Impute Nulls"
   ],
   "id": "t3qCixEzhfhi"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "YsZPpmjOlXrJ"
   },
   "outputs": [],
   "source": [
    "def make_feature_df(data, categorical_columns, numerical_columns):\n",
    "    \"\"\"\n",
    "    Return a dataset made by one-hot encoding for categorical columns and concatenate with numerical columns\n",
    "    \"\"\"\n",
    "    feature_df = pd.get_dummies(data[categorical_columns], columns=categorical_columns)\n",
    "    for col in numerical_columns:\n",
    "        if col in data.columns:\n",
    "            feature_df[col] = data[col]\n",
    "    return feature_df"
   ],
   "id": "YsZPpmjOlXrJ"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "hOhKuKnslZsz"
   },
   "outputs": [],
   "source": [
    "corrupted_data_features = make_feature_df(corrupted_data, categorical_columns = COLUMN_TO_TYPE['categorical'],\n",
    "                                   numerical_columns = COLUMN_TO_TYPE['numerical'])"
   ],
   "id": "hOhKuKnslZsz"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wZGsq8o-lsfx",
    "outputId": "c5edf6c7-4ac6-4f88-9640-33c99e278e47"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['SCHL',\n",
       " 'MAR',\n",
       " 'RELP',\n",
       " 'DIS',\n",
       " 'ESP',\n",
       " 'CIT',\n",
       " 'MIG',\n",
       " 'MIL',\n",
       " 'ANC',\n",
       " 'NATIVITY',\n",
       " 'DEAR',\n",
       " 'DEYE',\n",
       " 'DREM',\n",
       " 'SEX',\n",
       " 'RAC1P']"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "input_columns = [col for col in corrupted_data.columns if col != 'AGEP']\n",
    "input_columns"
   ],
   "id": "wZGsq8o-lsfx"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "3T9JTTILx8U8"
   },
   "outputs": [],
   "source": [
    "indexes = corrupted_data[TARGET_COLUMN].isna()\n",
    "\n",
    "corrupted_data_train = corrupted_data[~indexes]\n",
    "corrupted_data_test = corrupted_data[indexes]"
   ],
   "id": "3T9JTTILx8U8"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bktcJMVwhfhj",
    "outputId": "b4ca3797-dfdb-432a-f97b-023606a9aa27"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start fitting\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:root:\n",
      "========== start: fit model\n",
      "WARNING:root:Already bound, ignoring bind()\n",
      "INFO:root:Epoch[0] Batch [0-5458]\tSpeed: 8160.17 samples/sec\tcross-entropy=4.665837\tAGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[0] Train-cross-entropy=4.245351\n",
      "INFO:root:Epoch[0] Train-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[0] Time cost=19.254\n",
      "INFO:root:Saved checkpoint to \"imputer_model/model-0000.params\"\n",
      "INFO:root:Epoch[0] Validation-cross-entropy=4.060982\n",
      "INFO:root:Epoch[0] Validation-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[1] Batch [0-5458]\tSpeed: 10354.05 samples/sec\tcross-entropy=4.094694\tAGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[1] Train-cross-entropy=4.093774\n",
      "INFO:root:Epoch[1] Train-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[1] Time cost=16.859\n",
      "INFO:root:Saved checkpoint to \"imputer_model/model-0001.params\"\n",
      "INFO:root:Epoch[1] Validation-cross-entropy=3.989524\n",
      "INFO:root:Epoch[1] Validation-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[2] Batch [0-5458]\tSpeed: 10297.03 samples/sec\tcross-entropy=4.033104\tAGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[2] Train-cross-entropy=4.056960\n",
      "INFO:root:Epoch[2] Train-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[2] Time cost=20.858\n",
      "INFO:root:Saved checkpoint to \"imputer_model/model-0002.params\"\n",
      "INFO:root:Epoch[2] Validation-cross-entropy=3.945636\n",
      "INFO:root:Epoch[2] Validation-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[3] Batch [0-5458]\tSpeed: 4940.77 samples/sec\tcross-entropy=4.008254\tAGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[3] Train-cross-entropy=4.040523\n",
      "INFO:root:Epoch[3] Train-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[3] Time cost=32.754\n",
      "INFO:root:Saved checkpoint to \"imputer_model/model-0003.params\"\n",
      "INFO:root:Epoch[3] Validation-cross-entropy=3.947219\n",
      "INFO:root:Epoch[3] Validation-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[4] Batch [0-5458]\tSpeed: 7394.38 samples/sec\tcross-entropy=3.996544\tAGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[4] Train-cross-entropy=4.025499\n",
      "INFO:root:Epoch[4] Train-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[4] Time cost=22.911\n",
      "INFO:root:Saved checkpoint to \"imputer_model/model-0004.params\"\n",
      "INFO:root:Epoch[4] Validation-cross-entropy=3.937765\n",
      "INFO:root:Epoch[4] Validation-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[5] Batch [0-5458]\tSpeed: 5754.77 samples/sec\tcross-entropy=3.984193\tAGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[5] Train-cross-entropy=4.017039\n",
      "INFO:root:Epoch[5] Train-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[5] Time cost=23.665\n",
      "INFO:root:Saved checkpoint to \"imputer_model/model-0005.params\"\n",
      "INFO:root:Epoch[5] Validation-cross-entropy=3.933068\n",
      "INFO:root:Epoch[5] Validation-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[6] Batch [0-5458]\tSpeed: 7922.61 samples/sec\tcross-entropy=3.974652\tAGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[6] Train-cross-entropy=4.007944\n",
      "INFO:root:Epoch[6] Train-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[6] Time cost=19.439\n",
      "INFO:root:Saved checkpoint to \"imputer_model/model-0006.params\"\n",
      "INFO:root:Epoch[6] Validation-cross-entropy=3.922740\n",
      "INFO:root:Epoch[6] Validation-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[7] Batch [0-5458]\tSpeed: 10424.03 samples/sec\tcross-entropy=3.967766\tAGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[7] Train-cross-entropy=4.002110\n",
      "INFO:root:Epoch[7] Train-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[7] Time cost=16.786\n",
      "INFO:root:Saved checkpoint to \"imputer_model/model-0007.params\"\n",
      "INFO:root:Epoch[7] Validation-cross-entropy=3.925354\n",
      "INFO:root:Epoch[7] Validation-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[8] Batch [0-5458]\tSpeed: 10265.91 samples/sec\tcross-entropy=3.961867\tAGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[8] Train-cross-entropy=3.996879\n",
      "INFO:root:Epoch[8] Train-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[8] Time cost=17.205\n",
      "INFO:root:Saved checkpoint to \"imputer_model/model-0008.params\"\n",
      "INFO:root:Epoch[8] Validation-cross-entropy=3.911712\n",
      "INFO:root:Epoch[8] Validation-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[9] Batch [0-5458]\tSpeed: 10280.96 samples/sec\tcross-entropy=3.958292\tAGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[9] Train-cross-entropy=3.992201\n",
      "INFO:root:Epoch[9] Train-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[9] Time cost=17.019\n",
      "INFO:root:Saved checkpoint to \"imputer_model/model-0009.params\"\n",
      "INFO:root:Epoch[9] Validation-cross-entropy=3.922457\n",
      "INFO:root:Epoch[9] Validation-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[10] Batch [0-5458]\tSpeed: 10332.86 samples/sec\tcross-entropy=3.954879\tAGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[10] Train-cross-entropy=3.991825\n",
      "INFO:root:Epoch[10] Train-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[10] Time cost=16.970\n",
      "INFO:root:Saved checkpoint to \"imputer_model/model-0010.params\"\n",
      "INFO:root:Epoch[10] Validation-cross-entropy=3.914961\n",
      "INFO:root:Epoch[10] Validation-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[11] Batch [0-5458]\tSpeed: 10342.43 samples/sec\tcross-entropy=3.947086\tAGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[11] Train-cross-entropy=3.987024\n",
      "INFO:root:Epoch[11] Train-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[11] Time cost=17.999\n",
      "INFO:root:Saved checkpoint to \"imputer_model/model-0011.params\"\n",
      "INFO:root:Epoch[11] Validation-cross-entropy=3.910263\n",
      "INFO:root:Epoch[11] Validation-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[12] Batch [0-5458]\tSpeed: 10313.90 samples/sec\tcross-entropy=3.948592\tAGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[12] Train-cross-entropy=3.986007\n",
      "INFO:root:Epoch[12] Train-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[12] Time cost=16.975\n",
      "INFO:root:Saved checkpoint to \"imputer_model/model-0012.params\"\n",
      "INFO:root:Epoch[12] Validation-cross-entropy=3.899879\n",
      "INFO:root:Epoch[12] Validation-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[13] Batch [0-5458]\tSpeed: 10352.62 samples/sec\tcross-entropy=3.946284\tAGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[13] Train-cross-entropy=3.984697\n",
      "INFO:root:Epoch[13] Train-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[13] Time cost=16.938\n",
      "INFO:root:Saved checkpoint to \"imputer_model/model-0013.params\"\n",
      "INFO:root:Epoch[13] Validation-cross-entropy=3.901365\n",
      "INFO:root:Epoch[13] Validation-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[14] Batch [0-5458]\tSpeed: 10275.77 samples/sec\tcross-entropy=3.943066\tAGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[14] Train-cross-entropy=3.981361\n",
      "INFO:root:Epoch[14] Train-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[14] Time cost=17.005\n",
      "INFO:root:Saved checkpoint to \"imputer_model/model-0014.params\"\n",
      "INFO:root:Epoch[14] Validation-cross-entropy=3.898815\n",
      "INFO:root:Epoch[14] Validation-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[15] Batch [0-5458]\tSpeed: 10322.59 samples/sec\tcross-entropy=3.941081\tAGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[15] Train-cross-entropy=3.982934\n",
      "INFO:root:Epoch[15] Train-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[15] Time cost=16.938\n",
      "INFO:root:Saved checkpoint to \"imputer_model/model-0015.params\"\n",
      "INFO:root:Epoch[15] Validation-cross-entropy=3.909409\n",
      "INFO:root:Epoch[15] Validation-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[16] Batch [0-5458]\tSpeed: 10405.52 samples/sec\tcross-entropy=3.937026\tAGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[16] Train-cross-entropy=3.979656\n",
      "INFO:root:Epoch[16] Train-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[16] Time cost=16.822\n",
      "INFO:root:Saved checkpoint to \"imputer_model/model-0016.params\"\n",
      "INFO:root:Epoch[16] Validation-cross-entropy=3.898946\n",
      "INFO:root:Epoch[16] Validation-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[17] Batch [0-5458]\tSpeed: 10332.21 samples/sec\tcross-entropy=3.937683\tAGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[17] Train-cross-entropy=3.977993\n",
      "INFO:root:Epoch[17] Train-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[17] Time cost=16.880\n",
      "INFO:root:Saved checkpoint to \"imputer_model/model-0017.params\"\n",
      "INFO:root:Epoch[17] Validation-cross-entropy=3.894288\n",
      "INFO:root:Epoch[17] Validation-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[18] Batch [0-5458]\tSpeed: 10331.01 samples/sec\tcross-entropy=3.936534\tAGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[18] Train-cross-entropy=3.979166\n",
      "INFO:root:Epoch[18] Train-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[18] Time cost=16.939\n",
      "INFO:root:Saved checkpoint to \"imputer_model/model-0018.params\"\n",
      "INFO:root:Epoch[18] Validation-cross-entropy=3.897898\n",
      "INFO:root:Epoch[18] Validation-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[19] Batch [0-5458]\tSpeed: 10293.55 samples/sec\tcross-entropy=3.935939\tAGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[19] Train-cross-entropy=3.976102\n",
      "INFO:root:Epoch[19] Train-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[19] Time cost=16.992\n",
      "INFO:root:Saved checkpoint to \"imputer_model/model-0019.params\"\n",
      "INFO:root:Epoch[19] Validation-cross-entropy=3.903155\n",
      "INFO:root:Epoch[19] Validation-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[20] Batch [0-5458]\tSpeed: 10242.03 samples/sec\tcross-entropy=3.933330\tAGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[20] Train-cross-entropy=3.974899\n",
      "INFO:root:Epoch[20] Train-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[20] Time cost=17.067\n",
      "INFO:root:Saved checkpoint to \"imputer_model/model-0020.params\"\n",
      "INFO:root:Epoch[20] Validation-cross-entropy=3.902145\n",
      "INFO:root:Epoch[20] Validation-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[21] Batch [0-5458]\tSpeed: 10288.52 samples/sec\tcross-entropy=3.934363\tAGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[21] Train-cross-entropy=3.973903\n",
      "INFO:root:Epoch[21] Train-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[21] Time cost=16.948\n",
      "INFO:root:Saved checkpoint to \"imputer_model/model-0021.params\"\n",
      "INFO:root:Epoch[21] Validation-cross-entropy=3.896361\n",
      "INFO:root:Epoch[21] Validation-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[22] Batch [0-5458]\tSpeed: 10251.30 samples/sec\tcross-entropy=3.932857\tAGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[22] Train-cross-entropy=3.975550\n",
      "INFO:root:Epoch[22] Train-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[22] Time cost=17.028\n",
      "INFO:root:Saved checkpoint to \"imputer_model/model-0022.params\"\n",
      "INFO:root:Epoch[22] Validation-cross-entropy=3.889575\n",
      "INFO:root:Epoch[22] Validation-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[23] Batch [0-5458]\tSpeed: 10242.56 samples/sec\tcross-entropy=3.932246\tAGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[23] Train-cross-entropy=3.975673\n",
      "INFO:root:Epoch[23] Train-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[23] Time cost=17.065\n",
      "INFO:root:Saved checkpoint to \"imputer_model/model-0023.params\"\n",
      "INFO:root:Epoch[23] Validation-cross-entropy=3.892252\n",
      "INFO:root:Epoch[23] Validation-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[24] Batch [0-5458]\tSpeed: 10377.06 samples/sec\tcross-entropy=3.931644\tAGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[24] Train-cross-entropy=3.973805\n",
      "INFO:root:Epoch[24] Train-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[24] Time cost=16.887\n",
      "INFO:root:Saved checkpoint to \"imputer_model/model-0024.params\"\n",
      "INFO:root:Epoch[24] Validation-cross-entropy=3.898134\n",
      "INFO:root:Epoch[24] Validation-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[25] Batch [0-5458]\tSpeed: 10179.42 samples/sec\tcross-entropy=3.930962\tAGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[25] Train-cross-entropy=3.974865\n",
      "INFO:root:Epoch[25] Train-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[25] Time cost=17.363\n",
      "INFO:root:Saved checkpoint to \"imputer_model/model-0025.params\"\n",
      "INFO:root:Epoch[25] Validation-cross-entropy=3.891967\n",
      "INFO:root:Epoch[25] Validation-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[26] Batch [0-5458]\tSpeed: 10237.87 samples/sec\tcross-entropy=3.930637\tAGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[26] Train-cross-entropy=3.970420\n",
      "INFO:root:Epoch[26] Train-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[26] Time cost=17.012\n",
      "INFO:root:Saved checkpoint to \"imputer_model/model-0026.params\"\n",
      "INFO:root:Epoch[26] Validation-cross-entropy=3.890735\n",
      "INFO:root:Epoch[26] Validation-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[27] Batch [0-5458]\tSpeed: 10211.24 samples/sec\tcross-entropy=3.928772\tAGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[27] Train-cross-entropy=3.971817\n",
      "INFO:root:Epoch[27] Train-AGEP-accuracy=0.000000\n",
      "INFO:root:Epoch[27] Time cost=17.036\n",
      "INFO:root:Saved checkpoint to \"imputer_model/model-0027.params\"\n",
      "INFO:root:No improvement detected for 5 epochs compared to 3.8895751696820113 last error obtained: 3.8998918897930688, stopping here\n",
      "INFO:root:\n",
      "========== done (552.4942972660065 s) fit model\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start prediction\n"
     ]
    }
   ],
   "source": [
    "import datawig\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# corrupted_data_train, corrupted_data_test = train_test_split(corrupted_data, test_size=0.2, random_state=SEED)\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer = datawig.SimpleImputer(\n",
    "    input_columns=input_columns, # column(s) containing information about the column we want to impute\n",
    "    output_column=TARGET_COLUMN, # the column we'd like to impute values for\n",
    "    output_path = 'imputer_model' # stores model data and metrics\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "print('Start fitting')\n",
    "imputer.fit(train_df=corrupted_data_train, num_epochs=50)\n",
    "\n",
    "#Impute missing values and return original dataframe with predictions\n",
    "print('Start prediction')\n",
    "imputed = imputer.predict(corrupted_data_test)"
   ],
   "id": "bktcJMVwhfhj"
  },
  {
   "cell_type": "code",
   "source": [
    "corrupted_data_test.AGEP.isna().sum()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SgYKNHE28f-J",
    "outputId": "29361075-ce72-48e4-86c0-ac6b48e49d4e"
   },
   "id": "SgYKNHE28f-J",
   "execution_count": 45,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2937"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "imputed.AGEP_imputed.isna().sum()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1N2oeOnx7pnj",
    "outputId": "c96d50a3-ae70-4545-98e2-39d32b63e37d"
   },
   "id": "1N2oeOnx7pnj",
   "execution_count": 46,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "imputed.AGEP = imputed.AGEP_imputed.round()"
   ],
   "metadata": {
    "id": "HIPmOg9_9Ql0"
   },
   "id": "HIPmOg9_9Ql0",
   "execution_count": 47,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "imputed.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "VeiDEbvk9ELJ",
    "outputId": "5c74facb-0629-4b1c-9104-9cddab5583f1"
   },
   "id": "VeiDEbvk9ELJ",
   "execution_count": 48,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       AGEP  SCHL  MAR  RELP  DIS  ESP  CIT  MIG  MIL  ANC  NATIVITY  DEAR  \\\n",
       "10537  38.0  17.0    1    15    2  0.0    1  1.0  4.0    4         1     2   \n",
       "10538  28.0  16.0    5    15    2  0.0    1  1.0  4.0    4         1     2   \n",
       "10579  49.0  21.0    3    12    2  0.0    5  3.0  4.0    1         2     2   \n",
       "10580  18.0  13.0    5    12    2  0.0    5  3.0  0.0    1         2     2   \n",
       "10658  30.0  16.0    5    15    2  0.0    1  1.0  4.0    2         1     2   \n",
       "\n",
       "       DEYE  DREM  SEX  RAC1P  AGEP_imputed  \n",
       "10537     2   2.0    1      1     38.088773  \n",
       "10538     2   2.0    2      1     27.904971  \n",
       "10579     2   2.0    2      1     48.953131  \n",
       "10580     2   2.0    2      1     18.166863  \n",
       "10658     2   2.0    2      1     29.700971  "
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-3a91066f-4670-4f5a-912b-9087e8d67ba2\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGEP</th>\n",
       "      <th>SCHL</th>\n",
       "      <th>MAR</th>\n",
       "      <th>RELP</th>\n",
       "      <th>DIS</th>\n",
       "      <th>ESP</th>\n",
       "      <th>CIT</th>\n",
       "      <th>MIG</th>\n",
       "      <th>MIL</th>\n",
       "      <th>ANC</th>\n",
       "      <th>NATIVITY</th>\n",
       "      <th>DEAR</th>\n",
       "      <th>DEYE</th>\n",
       "      <th>DREM</th>\n",
       "      <th>SEX</th>\n",
       "      <th>RAC1P</th>\n",
       "      <th>AGEP_imputed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10537</th>\n",
       "      <td>38.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.088773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10538</th>\n",
       "      <td>28.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>27.904971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10579</th>\n",
       "      <td>49.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>48.953131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10580</th>\n",
       "      <td>18.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>18.166863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10658</th>\n",
       "      <td>30.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>29.700971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3a91066f-4670-4f5a-912b-9087e8d67ba2')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-3a91066f-4670-4f5a-912b-9087e8d67ba2 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-3a91066f-4670-4f5a-912b-9087e8d67ba2');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8HFGob3Vhfhj",
    "outputId": "012e2f64-6e22-4f6b-975a-2c0925f43ab4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MAE for regression - AGEP: 9.4\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[9.360912495743957]"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "evaluate_imputation(X_data, imputed, corrupted_data, ['AGEP'])"
   ],
   "id": "8HFGob3Vhfhj"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "ysio2qVchfhk"
   },
   "outputs": [],
   "source": [
    "imputed.to_csv('/content/drive/MyDrive/Colab Notebooks/NYU_Internship/results/imputed_with_datawig_df.csv', sep=\",\", columns=imputed.columns, float_format=\"%.4f\")"
   ],
   "id": "ysio2qVchfhk"
  },
  {
   "cell_type": "code",
   "source": [
    "data = X_data.copy(deep=True)\n",
    "data.loc[indexes, TARGET_COLUMN] = imputed\n",
    "data.to_csv('/content/drive/MyDrive/Colab Notebooks/NYU_Internship/results/imputed_with_datawig_full_df.csv', sep=\",\", columns=data.columns, float_format=\"%.4f\")"
   ],
   "metadata": {
    "id": "3vtpSKTvCLkM"
   },
   "id": "3vtpSKTvCLkM",
   "execution_count": 58,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!ls ../"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rfGQpatw-Ts2",
    "outputId": "826c56cc-e568-4c20-d941-0ef717b21371"
   },
   "id": "rfGQpatw-Ts2",
   "execution_count": 53,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data  imputer_model  sample_data\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    ""
   ],
   "metadata": {
    "id": "U41QCb6_BZLN"
   },
   "id": "U41QCb6_BZLN",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "DataWig_Usage.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}